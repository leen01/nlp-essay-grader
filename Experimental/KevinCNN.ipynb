{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/site-packages (4.7.0)\r\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (1.11.0)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (1.2.0)\r\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (0.9.0)\r\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (2.0.1)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (1.16.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (2.28.1)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\r\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (3.19.5)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (4.64.1)\r\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (1.23.2)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/site-packages (from tensorflow-datasets) (0.3.6)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2022.9.14)\r\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/site-packages (from etils[epath]->tensorflow-datasets) (3.11.0)\r\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/site-packages (from etils[epath]->tensorflow-datasets) (4.3.0)\r\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/site-packages (from etils[epath]->tensorflow-datasets) (5.10.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.57.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m22.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3.10 -m pip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install tensorflow-datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kevin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "# import string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# set display settings\n",
    "# pd.set_option(\"max_columns\", None)\n",
    "# pd.set_option('max_colwidth', None)\n",
    "# pd.set_option(\"expand_frame_repr\", False)"
   ],
   "metadata": {
    "id": "t6s5gHh15a_3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 173,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# HELPER FUNCTIONS\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "# clean the text\n",
    "def pre_processor(text):\n",
    "    # no_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return re.sub('[\\W]+', ' ', text.lower())\n",
    "\n",
    "def split_X_y(df):\n",
    "  y = df.pop(\"vocabulary\")\n",
    "  # y['text_id'] = df['text_id']\n",
    "  X = df.drop(['cohesion', 'syntax', 'phraseology', 'grammar', 'conventions'], axis=1)\n",
    "  return X, y\n"
   ],
   "metadata": {
    "id": "PE87hh08NNE7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 174,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# LOAD DATA FROM CSV FILES\n",
    "def get_data(train_path, random_state):\n",
    "  tf.random.set_seed(random_state)\n",
    "\n",
    "  # prepare the training and testing and data\n",
    "  df_raw = pd.read_csv(train_path)\n",
    "  X, y = split_X_y(df_raw)\n",
    "\n",
    "  # clean the data\n",
    "  X = clean_data(X)\n",
    "\n",
    "  # store the dataframe as a tensorflow dataset\n",
    "  data_tf = tf.data.Dataset.from_tensor_slices(\n",
    "      (X['full_text'].values, y.values)\n",
    "  )\n",
    "\n",
    "  # separate the data into train, test, and validation groups\n",
    "  splits=[0.6, 0.2, 0.2]\n",
    "  data_tf_test = data_tf.take(int(df_raw.shape[0]*splits[2]))\n",
    "  data_tf_train_valid = data_tf.skip(int(df_raw.shape[0]*splits[2]))\n",
    "  data_tf_train = data_tf_train_valid.take(int(df_raw.shape[0]*splits[0]))\n",
    "  data_tf_valid = data_tf_train_valid.skip(int(df_raw.shape[0]*splits[0]))\n",
    "\n",
    "  # return X_train, X_test, y_train, y_test\n",
    "  return data_tf_test, data_tf_train, data_tf_valid"
   ],
   "metadata": {
    "id": "4ImA-y-a4Y1H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 175,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# CLEAN DATA\n",
    "def clean_data(X):\n",
    "    X['full_text'] = X['full_text'].apply(pre_processor)\n",
    "\n",
    "    # count words\n",
    "    X['words'] = X['full_text'].apply(lambda x: [word for word in x.split()])\n",
    "    # df['num_full_words'] = df['full_words'].apply(lambda x: len(x))\n",
    "    # df['num_cleaned_words'] = df['cleaned_text'].apply(lambda x: len(x))\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    X['words'] = X['words'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    X['num_words'] = X['words'].apply(lambda x: len(x))\n",
    "    # df.head()\n",
    "    # df.hist(column='num_words')\n",
    "    X['full_text'] = X['words'].apply(lambda x: \" \".join(x))\n",
    "    return X"
   ],
   "metadata": {
    "id": "qG0e898ZZamv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 176,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# CREATE VOCABULARY BY COUNTING WORD OCCURRENCES\n",
    "def get_vocabulary(data_tf_train):\n",
    "  # try and except the TF tokenizer\n",
    "  # try:\n",
    "  #     tokenizer = tfds.features.text.Tokenizer()\n",
    "  # except AttributeError:\n",
    "  tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "  # create an instance of the Counter class\n",
    "  token_counts = Counter()\n",
    "\n",
    "  for example in data_tf_train:\n",
    "      tokens = tokenizer.tokenize(example[0].numpy())\n",
    "      token_counts.update(tokens)\n",
    "      \n",
    "  print('Size of training vocabulary:', len(token_counts))\n",
    "  return token_counts\n",
    "  # display(token_counts)"
   ],
   "metadata": {
    "id": "1qgA3Gfe2kNq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 186,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# CREATE EMBEDDING BY ENCODING TEXT\n",
    "def get_encoding(data_tf_train, data_tf_valid, data_tf_test, token_counts):\n",
    "  # define function for token encoder\n",
    "  def encode(text_tensor, label):\n",
    "      text = text_tensor.numpy()[0]\n",
    "      encoded_text = encoder.encode(text)\n",
    "      return encoded_text, label\n",
    "\n",
    "  #  wrap the encode function to a TF Operator\n",
    "  def encode_map_fn(text, label):\n",
    "      return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "  # # create an instance of the TF encoder class\n",
    "  # try:\n",
    "  #     # token_counts contains our training vocabulary\n",
    "  #     encoder = tfds.features.text.TokenTextEncoder(token_counts)\n",
    "  # except AttributeError:\n",
    "  #     encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "  encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "\n",
    "  data_tf_train = data_tf_train.map(encode_map_fn, encoder)\n",
    "  data_tf_valid = data_tf_valid.map(encode_map_fn, encoder)\n",
    "  data_tf_test = data_tf_test.map(encode_map_fn, encoder)\n",
    "  return data_tf_train, data_tf_valid, data_tf_test"
   ],
   "metadata": {
    "id": "Onc1xZcra8uN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 190,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # BATCH DATA\n",
    "# def get_batches(batch_size, X_train, X_test, y_train):\n",
    "#   data_tf_train = data_tf_train.map(encode_map_fn)\n",
    "#\n",
    "#   data_tf = tf.data.Dataset.from_tensor_slices(\n",
    "#       (X_train[['encoding']].values, y_train.values)\n",
    "#   )\n",
    "#\n",
    "#   # # divide the data into mini-batches of size 3\n",
    "#   # data_tf_batched = X_train.padded_batch(3, padded_shapes=([-1], []))\n",
    "#\n",
    "#   # for batch in data_tf_batched:\n",
    "#   #     print('Batch dimension:', batch[0].shape)\n",
    "#\n",
    "#   # for example in data_tf_batched.take(1):\n",
    "#   #     print(example[0].numpy())\n",
    "#   # return train_data, test_data\n",
    "#   return 0, 0\n"
   ],
   "metadata": {
    "id": "VZ8QSQWddEQv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 191,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "  # initialize paths to csv files\n",
    "  train_path = 'train.csv'\n",
    "\n",
    "  # ensure the state is repeatable\n",
    "  random_state=16\n",
    "\n",
    "  # define batch size\n",
    "  batch_size = 32\n",
    "\n",
    "  # get the dataframes\n",
    "  data_tf_test, data_tf_train, data_tf_valid = get_data(train_path, random_state)\n",
    "\n",
    "  # get the unique words used\n",
    "  token_counts = get_vocabulary(data_tf_train)\n",
    "  # display(token_counts)\n",
    "\n",
    "  data_tf_train, data_tf_valid, data_tf_test = get_encoding(data_tf_train, data_tf_valid, data_tf_test, token_counts)\n",
    "\n",
    "  for example in data_tf_train.take(2):\n",
    "    print ('----------------------------')\n",
    "    print('Sequence length:', example[0].shape)\n",
    "    print('Integer sequence:\\n', example[0].numpy())\n",
    "\n",
    "\n",
    "  # # inspection of the first 5 examples\n",
    "  # for example in data_tf_test.take(5):\n",
    "  #     # print review (first 40 characters) and sentiment (label)\n",
    "  #     tf.print(example[0], example[1])\n",
    "  #\n",
    "  # # inspection of the first 5 examples\n",
    "  # for example in data_tf_train.take(5):\n",
    "  #     # print review (first 40 characters) and sentiment (label)\n",
    "  #     tf.print(example[0], example[1])\n",
    "  #\n",
    "  # # inspection of the first 5 examples\n",
    "  # for example in data_tf_valid.take(5):\n",
    "  #     # print review (first 40 characters) and sentiment (label)\n",
    "  #     tf.print(example[0], example[1])\n",
    "\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "bNmb8Q70Ycsx",
    "outputId": "82d980a4-d30b-46e3-9578-d26c54c18a34",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 192,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training vocabulary: 16020\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<TokenTextEncoder vocab_size=16022>) with an unsupported type (<class 'tensorflow_datasets.core.deprecated.text.text_encoder.TokenTextEncoder'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [192]\u001B[0m, in \u001B[0;36m<cell line: 42>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInteger sequence:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m, example[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m     26\u001B[0m   \u001B[38;5;66;03m# # inspection of the first 5 examples\u001B[39;00m\n\u001B[1;32m     27\u001B[0m   \u001B[38;5;66;03m# for example in data_tf_test.take(5):\u001B[39;00m\n\u001B[1;32m     28\u001B[0m   \u001B[38;5;66;03m#     # print review (first 40 characters) and sentiment (label)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     38\u001B[0m   \u001B[38;5;66;03m#     # print review (first 40 characters) and sentiment (label)\u001B[39;00m\n\u001B[1;32m     39\u001B[0m   \u001B[38;5;66;03m#     tf.print(example[0], example[1])\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [192]\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m token_counts \u001B[38;5;241m=\u001B[39m get_vocabulary(data_tf_train)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# display(token_counts)\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m data_tf_train, data_tf_valid, data_tf_test \u001B[38;5;241m=\u001B[39m \u001B[43mget_encoding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_tf_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_tf_valid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_tf_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_counts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m data_tf_train\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m     21\u001B[0m   \u001B[38;5;28mprint\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m----------------------------\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [190]\u001B[0m, in \u001B[0;36mget_encoding\u001B[0;34m(data_tf_train, data_tf_valid, data_tf_test, token_counts)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# # create an instance of the TF encoder class\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# try:\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m#     # token_counts contains our training vocabulary\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#     encoder = tfds.features.text.TokenTextEncoder(token_counts)\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# except AttributeError:\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m#     encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\u001B[39;00m\n\u001B[1;32m     19\u001B[0m encoder \u001B[38;5;241m=\u001B[39m tfds\u001B[38;5;241m.\u001B[39mdeprecated\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mTokenTextEncoder(token_counts)\n\u001B[0;32m---> 21\u001B[0m data_tf_train \u001B[38;5;241m=\u001B[39m \u001B[43mdata_tf_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencode_map_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m data_tf_valid \u001B[38;5;241m=\u001B[39m data_tf_valid\u001B[38;5;241m.\u001B[39mmap(encode_map_fn, encoder)\n\u001B[1;32m     23\u001B[0m data_tf_test \u001B[38;5;241m=\u001B[39m data_tf_test\u001B[38;5;241m.\u001B[39mmap(encode_map_fn, encoder)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2204\u001B[0m, in \u001B[0;36mDatasetV2.map\u001B[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001B[0m\n\u001B[1;32m   2202\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m MapDataset(\u001B[38;5;28mself\u001B[39m, map_func, preserve_cardinality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, name\u001B[38;5;241m=\u001B[39mname)\n\u001B[1;32m   2203\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2204\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mParallelMapDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2205\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2206\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2207\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2208\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2209\u001B[0m \u001B[43m      \u001B[49m\u001B[43mpreserve_cardinality\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2210\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5453\u001B[0m, in \u001B[0;36mParallelMapDataset.__init__\u001B[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001B[0m\n\u001B[1;32m   5451\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_deterministic \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   5452\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preserve_cardinality \u001B[38;5;241m=\u001B[39m preserve_cardinality\n\u001B[0;32m-> 5453\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_parallel_calls \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_parallel_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5455\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m   5456\u001B[0m variant_tensor \u001B[38;5;241m=\u001B[39m gen_dataset_ops\u001B[38;5;241m.\u001B[39mparallel_map_dataset_v2(\n\u001B[1;32m   5457\u001B[0m     input_dataset\u001B[38;5;241m.\u001B[39m_variant_tensor,  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   5458\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_func\u001B[38;5;241m.\u001B[39mfunction\u001B[38;5;241m.\u001B[39mcaptured_inputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5463\u001B[0m     preserve_cardinality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preserve_cardinality,\n\u001B[1;32m   5464\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_common_args)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001B[0m, in \u001B[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    181\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m Trace(trace_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrace_kwargs):\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1638\u001B[0m, in \u001B[0;36mconvert_to_tensor\u001B[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001B[0m\n\u001B[1;32m   1629\u001B[0m       \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1630\u001B[0m           _add_error_prefix(\n\u001B[1;32m   1631\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConversion function \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconversion_func\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m for type \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1634\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactual = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mret\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mbase_dtype\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1635\u001B[0m               name\u001B[38;5;241m=\u001B[39mname))\n\u001B[1;32m   1637\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1638\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mconversion_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_ref\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mas_ref\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1640\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[1;32m   1641\u001B[0m   \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:343\u001B[0m, in \u001B[0;36m_constant_tensor_conversion_function\u001B[0;34m(v, dtype, name, as_ref)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_constant_tensor_conversion_function\u001B[39m(v, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    341\u001B[0m                                          as_ref\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    342\u001B[0m   _ \u001B[38;5;241m=\u001B[39m as_ref\n\u001B[0;32m--> 343\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconstant\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001B[0m, in \u001B[0;36mconstant\u001B[0;34m(value, dtype, shape, name)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconstant\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconstant\u001B[39m(value, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConst\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    172\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mallow_broadcast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:279\u001B[0m, in \u001B[0;36m_constant_impl\u001B[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001B[0m\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m trace\u001B[38;5;241m.\u001B[39mTrace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.constant\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    278\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001B[0;32m--> 279\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_constant_eager_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m g \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mget_default_graph()\n\u001B[1;32m    282\u001B[0m tensor_value \u001B[38;5;241m=\u001B[39m attr_value_pb2\u001B[38;5;241m.\u001B[39mAttrValue()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:304\u001B[0m, in \u001B[0;36m_constant_eager_impl\u001B[0;34m(ctx, value, dtype, shape, verify_shape)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_constant_eager_impl\u001B[39m(ctx, value, dtype, shape, verify_shape):\n\u001B[1;32m    303\u001B[0m   \u001B[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 304\u001B[0m   t \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_to_eager_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[1;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: Attempt to convert a value (<TokenTextEncoder vocab_size=16022>) with an unsupported type (<class 'tensorflow_datasets.core.deprecated.text.text_encoder.TokenTextEncoder'>) to a Tensor."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# divide the data into mini-batches of size 3\n",
    "\n",
    "data_tf_batched = data_tf_train_subset.padded_batch(3, padded_shapes=([-1], []))\n",
    "\n",
    "for batch in data_tf_batched:\n",
    "    print('Batch dimension:', batch[0].shape)\n",
    "\n",
    "for example in data_tf_batched.take(1):\n",
    "    print(example[0].numpy())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "IZ7DcoPtPB-b",
    "outputId": "f916266c-b40c-45c2-8617-151a1b5249c5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}