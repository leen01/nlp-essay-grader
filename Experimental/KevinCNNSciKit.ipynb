{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "# import string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# set display settings\n",
    "# pd.set_option(\"max_columns\", None)\n",
    "# pd.set_option('max_colwidth', None)\n",
    "# pd.set_option(\"expand_frame_repr\", False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "# define function for token encoder\n",
    "def encode(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, label\n",
    "\n",
    "#  wrap the encode function to a TF Operator\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "# clean the text\n",
    "def pre_processor(text):\n",
    "    # no_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return re.sub('[\\W]+', ' ', text.lower())\n",
    "\n",
    "def split_X_y(df):\n",
    "  y = df.pop(\"vocabulary\")\n",
    "  # y['text_id'] = df['text_id']\n",
    "  X = df.drop(['cohesion', 'syntax', 'phraseology', 'grammar', 'conventions'], axis=1)\n",
    "  return X, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# LOAD DATA FROM CSV FILES\n",
    "def get_data(train_path, random_state):\n",
    "  tf.random.set_seed(random_state)\n",
    "\n",
    "  # prepare the training and testing and data\n",
    "  df_train_raw = pd.read_csv(train_path)\n",
    "  X, y = split_X_y(df_train_raw)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# CLEAN DATA\n",
    "def clean_data(X):\n",
    "    X['full_text'] = X['full_text'].apply(pre_processor)\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# CREATE VOCABULARY BY COUNTING WORD OCCURRENCES\n",
    "def get_vocabulary(X_train):\n",
    "  # count words\n",
    "  X_train['words'] = X_train['full_text'].apply(lambda x: [word for word in x.split()])\n",
    "  # df['num_full_words'] = df['full_words'].apply(lambda x: len(x))\n",
    "  # df['num_cleaned_words'] = df['cleaned_text'].apply(lambda x: len(x))\n",
    "  # remove stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  X_train['words'] = X_train['words'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  X_train['num_words'] = X_train['words'].apply(lambda x: len(x))\n",
    "  # df.head()\n",
    "  # df.hist(column='num_words')\n",
    "\n",
    "  # try and except the TF tokenizer\n",
    "  try:\n",
    "      tokenizer = tfds.features.text.Tokenizer()\n",
    "  except AttributeError:\n",
    "      tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "  # create an instance of the Counter class\n",
    "  token_counts = Counter()\n",
    "\n",
    "  for x in X_train['full_text']:\n",
    "      tokens = tokenizer.tokenize(x)\n",
    "      token_counts.update(tokens)\n",
    "      \n",
    "  print('Size of training vocabulary:', len(token_counts))\n",
    "  return token_counts\n",
    "  # display(token_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# CREATE EMBEDDING BY ENCODING TEXT\n",
    "def get_encoding(X, token_counts):\n",
    "  # create an instance of the TF encoder class\n",
    "  try:\n",
    "      # token_counts contains our training vocabulary\n",
    "      encoder = tfds.features.text.TokenTextEncoder(token_counts)\n",
    "  except AttributeError:\n",
    "      encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "\n",
    "  # X['encoding'] = X['full_text'].apply(lambda x: encoder.encode(x))\n",
    "  # return X\n",
    "  data_tf_train = X.map(encode_map_fn)\n",
    "  return data_tf_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# BATCH DATA\n",
    "def get_batches(batch_size, X_train, X_test, y_train):\n",
    "  data_tf_train = data_tf_train.map(encode_map_fn)\n",
    "\n",
    "\n",
    "  data_tf = tf.data.Dataset.from_tensor_slices(\n",
    "      (X_train[['encoding']].values, y_train.values)\n",
    "  )\n",
    "\n",
    "  # # divide the data into mini-batches of size 3\n",
    "  # data_tf_batched = X_train.padded_batch(3, padded_shapes=([-1], []))\n",
    "\n",
    "  # for batch in data_tf_batched:\n",
    "  #     print('Batch dimension:', batch[0].shape)\n",
    "\n",
    "  # for example in data_tf_batched.take(1):\n",
    "  #     print(example[0].numpy())\n",
    "  # return train_data, test_data\n",
    "  return 0, 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - '/Users/kevin/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 84\u001B[0m     root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mzip_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/nltk/data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords.zip/stopwords/\u001B[0m\n\n  Searched in:\n    - '/Users/kevin/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[0;32mIn [46]\u001B[0m, in \u001B[0;36m<cell line: 31>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m       display(token_counts)\n\u001B[1;32m     23\u001B[0m   \u001B[38;5;66;03m# X_train = get_encoding(train_tf, token_counts)\u001B[39;00m\n\u001B[1;32m     24\u001B[0m   \u001B[38;5;66;03m# display(X_train.head())\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \n\u001B[1;32m     26\u001B[0m \n\u001B[1;32m     27\u001B[0m \n\u001B[1;32m     28\u001B[0m   \u001B[38;5;66;03m# train_data, test_data = get_batches(batch_size, X_train, X_test, y_train)\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [46]\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m X_train \u001B[38;5;241m=\u001B[39m clean_data(X_train)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# get the unique words used\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m token_counts \u001B[38;5;241m=\u001B[39m \u001B[43mget_vocabulary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pd\u001B[38;5;241m.\u001B[39moption_context(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdisplay.max_colwidth\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     21\u001B[0m     display(token_counts)\n",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36mget_vocabulary\u001B[0;34m(X_train)\u001B[0m\n\u001B[1;32m      4\u001B[0m X_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m X_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfull_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m x\u001B[38;5;241m.\u001B[39msplit()])\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# df['num_full_words'] = df['full_words'].apply(lambda x: len(x))\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# df['num_cleaned_words'] = df['cleaned_text'].apply(lambda x: len(x))\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# remove stop words\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m stop_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[43mstopwords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      9\u001B[0m X_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m X_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words])\n\u001B[1;32m     10\u001B[0m X_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_words\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m X_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mlen\u001B[39m(x))\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__getattr__\u001B[0;34m(self, attr)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLazyCorpusLoader object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     84\u001B[0m             root \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubdir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzip_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n\u001B[0;32m---> 86\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# Load the corpus.\u001B[39;00m\n\u001B[1;32m     89\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__reader_cls(root, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs)\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 81\u001B[0m         root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/site-packages/nltk/data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[1;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - '/Users/kevin/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/usr/local/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  # initialize paths to csv files\n",
    "  train_path = 'train.csv'\n",
    "\n",
    "  # ensure the state is repeatable\n",
    "  random_state=16\n",
    "\n",
    "  # define batch size\n",
    "  batch_size = 32\n",
    "\n",
    "  # get the dataframes\n",
    "  X_train, X_test, y_train, y_test = get_data(train_path, random_state)\n",
    "\n",
    "  # clean the training data\n",
    "  X_train = clean_data(X_train)\n",
    "\n",
    "  # get the unique words used\n",
    "  token_counts = get_vocabulary(X_train)\n",
    "\n",
    "  with pd.option_context('display.max_colwidth', None):\n",
    "      display(token_counts)\n",
    "\n",
    "  # X_train = get_encoding(train_tf, token_counts)\n",
    "  # display(X_train.head())\n",
    "\n",
    "\n",
    "\n",
    "  # train_data, test_data = get_batches(batch_size, X_train, X_test, y_train)\n",
    "\n",
    "\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}